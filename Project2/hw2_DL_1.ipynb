{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BXNw7k55GYKX"
   },
   "outputs": [],
   "source": [
    "from turtle import width\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "L7JUXjQ9Iixy"
   },
   "outputs": [],
   "source": [
    "class ImagePixelDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self,image_path):\n",
    "    self.image = np.array(Image.open(image_path))\n",
    "    self.height,self.width,_ = self.image.shape\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.height * self.width\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "    y = idx // self.width\n",
    "    x = idx % self.width\n",
    "    color = self.image[y,x]/255\n",
    "\n",
    "    y/= self.height\n",
    "    x/= self.width\n",
    "\n",
    "    return np.array([x,y]), color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fYiCBtGRW-eM",
    "outputId": "8ab16046-dbfb-40b6-dbfd-c60e714785d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "path = \"/content/drive/My Drive/sifnos-greece-3840x2160-12799.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GZmBqWpK6cNk"
   },
   "outputs": [],
   "source": [
    "dataset = ImagePixelDataset(path)\n",
    "\n",
    "dataloader = DataLoader(dataset,batch_size=16,shuffle=True)\n",
    "train_feature, train_labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TtcL8tndYQVb"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class MLP3(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1,hidden_size2, output_size):\n",
    "        super(MLP3, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class MLP4(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1,hidden_size2,hidden_size3, output_size):\n",
    "        super(MLP4, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size3, output_size)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.relu4(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TBEabcgFZ-D4"
   },
   "outputs": [],
   "source": [
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRwuULy7YrkG"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8GYNpGZ-aJRl"
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2uL02-39aRoU"
   },
   "outputs": [],
   "source": [
    "input_size = 2 # Define your input size based on the number of pixels in an image\n",
    "hidden_size = 256  # You can adjust this based on your requirements\n",
    "output_size = 3  # Assuming RGB prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoPfXX09zP3v"
   },
   "outputs": [],
   "source": [
    "model_256 = MLP(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahHk9PlLa2ep"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model_256.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lb9nTulcFFIH"
   },
   "source": [
    "Training 2 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y14k1wDEobUe",
    "outputId": "2f333369-c689-4ed8-e9b5-78cb1da702ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/414720 [00:00<?, ?it/s]<ipython-input-24-fe6654b23c18>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-24-fe6654b23c18>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n",
      "100%|██████████| 414720/414720 [07:55<00:00, 871.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.0599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414720/414720 [07:55<00:00, 872.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.1038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414720/414720 [07:43<00:00, 894.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.0432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in tqdm(train_dataloader):\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_256(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UwDOAy2obXg",
    "outputId": "cf2365e0-e8cf-4974-e97e-a1631f2baeab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-0da2bb3ac985>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-25-0da2bb3ac985>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1075\n"
     ]
    }
   ],
   "source": [
    "model_256.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, targets in test_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        outputs = model_256(inputs)\n",
    "\n",
    "        test_loss += criterion(outputs, targets)\n",
    "\n",
    "    average_test_loss = test_loss / len(test_dataloader)\n",
    "    print(f'Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_H271yUXIro"
   },
   "outputs": [],
   "source": [
    "torch.save(model_256, '/content/drive/My Drive/model_mlp_2_256_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQ7VF28Jah4Z",
    "outputId": "f4af37c2-823a-4877-ac6c-71c9e2804259"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-bdd342f0fc17>:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  coor = torch.tensor(coor)\n"
     ]
    }
   ],
   "source": [
    "coor = [dataset[i][0] for i in range(len(dataset))]\n",
    "coor = torch.tensor(coor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Le-acBgYGoZx",
    "outputId": "abf6b726-8237-411e-86cd-3685755c8eb4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-b9e9c0f3fc68>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "with torch.no_grad():\n",
    "  pre = []\n",
    "  for i in range(0, coor.size(0), batch_size):\n",
    "    batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n",
    "    batch_pred = model_256(batch_coor)\n",
    "    pre.append(batch_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eRLWG2PnuA8"
   },
   "outputs": [],
   "source": [
    "predicted_rgb = torch.cat(pre, dim=0)\n",
    "\n",
    "predicted_rgb = predicted_rgb.view(dataset.height, dataset.width, 3).numpy() * 255\n",
    "predicted_image = Image.fromarray(predicted_rgb.astype('uint8'))\n",
    "\n",
    "predicted_image.save('./mlp_2_256_3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WgLHJfmLQTZ"
   },
   "source": [
    "Training 3 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3da9sgcobwy",
    "outputId": "3b1b3751-c929-44e7-eacc-7e416e2bb729"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-64d3c340710c>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-10-64d3c340710c>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.0357\n",
      "Epoch 2/2, Loss: 0.0202\n"
     ]
    }
   ],
   "source": [
    "input_size = 2 # Define your input size based on the number of pixels in an image\n",
    "output_size = 3  # Assuming RGB prediction\n",
    "\n",
    "\n",
    "model_256_128 = MLP3(input_size,256,128, output_size)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_256_128.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_256_128(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mWqeuDne0xkO",
    "outputId": "65dfe7d5-b389-4a67-b106-482fc2e760d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-25bffee3307f>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-11-25bffee3307f>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0380\n"
     ]
    }
   ],
   "source": [
    "model_256_128.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, targets in test_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        outputs = model_256_128(inputs)\n",
    "\n",
    "        test_loss += criterion(outputs, targets)\n",
    "\n",
    "    average_test_loss = test_loss / len(test_dataloader)\n",
    "    print(f'Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3cPcR3-1pGE"
   },
   "outputs": [],
   "source": [
    "torch.save(model_256_128, '/content/drive/My Drive/model_mlp_2_256_128_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-8QK0u3147l",
    "outputId": "5d250c21-2a02-49fd-f020-7eb16b82e6ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-39939a512046>:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  coor = torch.tensor(coor)\n",
      "<ipython-input-13-39939a512046>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "coor = [dataset[i][0] for i in range(len(dataset))]\n",
    "coor = torch.tensor(coor)\n",
    "\n",
    "batch_size = 64\n",
    "with torch.no_grad():\n",
    "  pre = []\n",
    "  for i in range(0, coor.size(0), batch_size):\n",
    "    batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n",
    "    batch_pred = model_256_128(batch_coor)\n",
    "    pre.append(batch_pred)\n",
    "\n",
    "predicted_rgb = torch.cat(pre, dim=0)\n",
    "\n",
    "predicted_rgb = predicted_rgb.view(dataset.height, dataset.width, 3).numpy() * 255\n",
    "predicted_image = Image.fromarray(predicted_rgb.astype('uint8'))\n",
    "\n",
    "predicted_image.save('./mlp_2_256_128_3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3KJ0XgJO4_E"
   },
   "source": [
    "Training 4 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ngMj9H11__R",
    "outputId": "97b97a31-bc19-44ab-f355-b6b2434013e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-5f213a06931f>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-35-5f213a06931f>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.0385\n",
      "Epoch 2/2, Loss: 0.0713\n"
     ]
    }
   ],
   "source": [
    "input_size = 2 # Define your input size based on the number of pixels in an image\n",
    "output_size = 3  # Assuming RGB prediction\n",
    "\n",
    "\n",
    "model_256_128_64 = MLP4(input_size,256,128,64, output_size)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model_256_128_64.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_256_128_64(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BALCcIXM_eZM",
    "outputId": "7029c537-ba05-413d-ee8d-e351630d6ac9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-acf6ac663735>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-36-acf6ac663735>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0724\n"
     ]
    }
   ],
   "source": [
    "model_256_128_64.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, targets in test_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        outputs = model_256_128_64(inputs)\n",
    "\n",
    "        test_loss += criterion(outputs, targets)\n",
    "\n",
    "    average_test_loss = test_loss / len(test_dataloader)\n",
    "    print(f'Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6SQ0f3f_ecC"
   },
   "outputs": [],
   "source": [
    "torch.save(model_256_128_64, '/content/drive/My Drive/model_mlp_2_256_128_64_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rP5g-VjS_ee9",
    "outputId": "cfb46890-82e1-4803-cf9b-d36b47489c93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-4a7a10a82288>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "coor = [dataset[i][0] for i in range(len(dataset))]\n",
    "coor = torch.tensor(coor)\n",
    "\n",
    "batch_size = 64\n",
    "with torch.no_grad():\n",
    "  pre = []\n",
    "  for i in range(0, coor.size(0), batch_size):\n",
    "    batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n",
    "    batch_pred = model_256_128_64(batch_coor)\n",
    "    pre.append(batch_pred)\n",
    "\n",
    "predicted_rgb = torch.cat(pre, dim=0)\n",
    "\n",
    "predicted_rgb = predicted_rgb.view(dataset.height, dataset.width, 3).numpy() * 255\n",
    "predicted_image = Image.fromarray(predicted_rgb.astype('uint8'))\n",
    "\n",
    "predicted_image.save('./mlp_2_256_128_64_3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uef56kmbLcgK",
    "outputId": "45400e7c-1226-4dad-99fa-8935d8d9f491"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-1a7634670891>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-39-1a7634670891>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.0937\n",
      "Epoch 2/2, Loss: 0.0900\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "input_size = 2 # Define your input size based on the number of pixels in an image\n",
    "output_size = 3  # Assuming RGB prediction\n",
    "\n",
    "\n",
    "model_256_128_64_b1024 = MLP4(input_size,256,128,64, output_size)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model_256_128_64_b1024.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_256_128_64_b1024(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQZDgp-9j4ez",
    "outputId": "b4517679-6b0a-4814-d168-23b3a9012fe3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-fe8a7c6bc17e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-40-fe8a7c6bc17e>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0921\n"
     ]
    }
   ],
   "source": [
    "model_256_128_64_b1024.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, targets in test_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        outputs = model_256_128_64_b1024(inputs)\n",
    "\n",
    "        test_loss += criterion(outputs, targets)\n",
    "\n",
    "    average_test_loss = test_loss / len(test_dataloader)\n",
    "    print(f'Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8y941I8dj4h5",
    "outputId": "b8ae294d-0aa2-4d37-c43b-98763061abfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-0fad2ac12d73>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "coor = [dataset[i][0] for i in range(len(dataset))]\n",
    "coor = torch.tensor(coor)\n",
    "\n",
    "batch_size = 64\n",
    "with torch.no_grad():\n",
    "  pre = []\n",
    "  for i in range(0, coor.size(0), batch_size):\n",
    "    batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n",
    "    batch_pred = model_256_128_64_b1024(batch_coor)\n",
    "    pre.append(batch_pred)\n",
    "\n",
    "predicted_rgb = torch.cat(pre, dim=0)\n",
    "\n",
    "predicted_rgb = predicted_rgb.view(dataset.height, dataset.width, 3).numpy() * 255\n",
    "predicted_image = Image.fromarray(predicted_rgb.astype('uint8'))\n",
    "\n",
    "predicted_image.save('./mlp_2_256_128_64_b1024.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvEASmK22uZ2"
   },
   "source": [
    "Higher Dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7fPVS3rA20X5"
   },
   "outputs": [],
   "source": [
    "from turtle import width\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "E8pVg3lXoRxZ"
   },
   "outputs": [],
   "source": [
    "class ImagePixelDatasetHigherDimention(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self,image_path,L):\n",
    "    self.image = np.array(Image.open(image_path))\n",
    "    self.height,self.width,_ = self.image.shape\n",
    "    self.L = L\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.height * self.width\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "    y = idx // self.width\n",
    "    x = idx % self.width\n",
    "    color = self.image[y,x]/255\n",
    "\n",
    "    y = np.interp(y,[0,self.height],[-1,1])\n",
    "    x = np.interp(x,[0,self.width],[-1,1])\n",
    "\n",
    "    input = []\n",
    "\n",
    "    for l in range(0,self.L):\n",
    "        input.append(np.sin(2**l * np.pi * x))\n",
    "        input.append(np.cos(2**l * np.pi * x))\n",
    "\n",
    "    for l in range(0,self.L):\n",
    "        input.append(np.sin(2**l * np.pi * y))\n",
    "        input.append(np.cos(2**l * np.pi * y))\n",
    "\n",
    "\n",
    "    return np.array(input), color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MO4Ew2M45VYH",
    "outputId": "cec729f5-8393-4514-caea-765fe26b810e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "path = \"/content/drive/My Drive/sifnos-greece-3840x2160-12799.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eHKwwjqM5Vau",
    "outputId": "7d7e60c7-fc3b-469d-c83d-6513e4a28fb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8983,  0.4394, -0.7894, -0.6139,  0.9692, -0.2462, -0.4772, -0.8788,\n",
      "         -0.7274, -0.6862,  0.9983, -0.0581, -0.1161, -0.9932,  0.2306,  0.9730],\n",
      "        [-0.5474, -0.8369,  0.9162,  0.4007,  0.7343, -0.6788, -0.9969, -0.0785,\n",
      "          0.5075,  0.8616,  0.8746,  0.4848,  0.8480, -0.5299, -0.8988, -0.4384],\n",
      "        [ 0.2683, -0.9633, -0.5169,  0.8560, -0.8850,  0.4656, -0.8241, -0.5664,\n",
      "          0.6884,  0.7254,  0.9986,  0.0523,  0.1045, -0.9945, -0.2079,  0.9781],\n",
      "        [-0.5127,  0.8586, -0.8804,  0.4743, -0.8351, -0.5501,  0.9188, -0.3947,\n",
      "          0.6691, -0.7431, -0.9945,  0.1045, -0.2079, -0.9781,  0.4067,  0.9135],\n",
      "        [ 1.0000,  0.0016,  0.0033, -1.0000, -0.0065,  1.0000, -0.0131,  0.9999,\n",
      "          0.6065,  0.7951,  0.9644,  0.2644,  0.5100, -0.8601, -0.8774,  0.4797],\n",
      "        [ 0.2683,  0.9633,  0.5169,  0.8560,  0.8850,  0.4656,  0.8241, -0.5664,\n",
      "          0.5025,  0.8646,  0.8689,  0.4950,  0.8601, -0.5100, -0.8774, -0.4797],\n",
      "        [ 0.0850, -0.9964, -0.1693,  0.9856, -0.3338,  0.9426, -0.6293,  0.7771,\n",
      "         -0.2924, -0.9563,  0.5592,  0.8290,  0.9272,  0.3746,  0.6947, -0.7193],\n",
      "        [-0.3675,  0.9300, -0.6836,  0.7299, -0.9979,  0.0654, -0.1305, -0.9914,\n",
      "         -0.9689, -0.2476,  0.4797, -0.8774, -0.8418,  0.5398, -0.9088, -0.4173],\n",
      "        [-0.4584, -0.8888,  0.8147,  0.5798,  0.9448, -0.3276, -0.6191, -0.7853,\n",
      "         -0.7173,  0.6967, -0.9996, -0.0291,  0.0581, -0.9983, -0.1161,  0.9932],\n",
      "        [ 0.7497,  0.6618,  0.9923, -0.1240, -0.2462, -0.9692,  0.4772,  0.8788,\n",
      "          0.1421, -0.9899, -0.2812,  0.9596, -0.5398,  0.8418, -0.9088,  0.4173],\n",
      "        [ 0.9536,  0.3012,  0.5745, -0.8185, -0.9404,  0.3400, -0.6394, -0.7688,\n",
      "          0.9436, -0.3311, -0.6248, -0.7808,  0.9757,  0.2193,  0.4279, -0.9038],\n",
      "        [-0.8954,  0.4452, -0.7973, -0.6036,  0.9625, -0.2714, -0.5225, -0.8526,\n",
      "          0.7214, -0.6926, -0.9992, -0.0407,  0.0814, -0.9967, -0.1622,  0.9868],\n",
      "        [ 0.4097, -0.9122, -0.7475,  0.6643, -0.9931, -0.1175,  0.2334, -0.9724,\n",
      "          0.8158, -0.5783, -0.9436, -0.3311,  0.6248, -0.7808, -0.9757,  0.2193],\n",
      "        [ 0.9574, -0.2887, -0.5528, -0.8333,  0.9214,  0.3887,  0.7163, -0.6978,\n",
      "          0.1132, -0.9936, -0.2250,  0.9744, -0.4384,  0.8988, -0.7880,  0.6157],\n",
      "        [ 0.6407,  0.7678,  0.9838,  0.1790,  0.3523, -0.9359, -0.6593,  0.7518,\n",
      "          0.8675,  0.4975,  0.8631, -0.5050, -0.8718, -0.4899,  0.8542, -0.5200],\n",
      "        [ 0.9453,  0.3261,  0.6165, -0.7873, -0.9708,  0.2398, -0.4656, -0.8850,\n",
      "         -0.8526,  0.5225, -0.8910, -0.4540,  0.8090, -0.5878, -0.9511, -0.3090]],\n",
      "       dtype=torch.float64) tensor([[0.4392, 0.4902, 0.5647],\n",
      "        [0.2000, 0.2118, 0.2392],\n",
      "        [0.0078, 0.1059, 0.3686],\n",
      "        [0.0000, 0.3412, 0.4510],\n",
      "        [0.0039, 0.1137, 0.2980],\n",
      "        [0.0078, 0.1608, 0.3294],\n",
      "        [0.5059, 0.4000, 0.4667],\n",
      "        [0.1020, 0.1765, 0.1529],\n",
      "        [0.9059, 0.6353, 0.3294],\n",
      "        [0.0941, 0.0941, 0.1412],\n",
      "        [0.0275, 0.1412, 0.2980],\n",
      "        [0.0000, 0.2392, 0.3216],\n",
      "        [0.1922, 0.4235, 0.6588],\n",
      "        [0.1176, 0.0941, 0.1020],\n",
      "        [0.0353, 0.2314, 0.4627],\n",
      "        [0.5490, 0.5882, 0.3804]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "L=4\n",
    "\n",
    "dataset = ImagePixelDatasetHigherDimention(path,L)\n",
    "dataloader = DataLoader(dataset,batch_size=16,shuffle=True)\n",
    "train_feature, train_labels = next(iter(dataloader))\n",
    "print(train_feature, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sb1v0jDX5Vdl"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1,hidden_size2,hidden_size3, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size3, output_size)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.relu4(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3FTAQfO5Vgu"
   },
   "outputs": [],
   "source": [
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxwMqxGz5ztP"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FCCuV8k26GgG",
    "outputId": "8889816a-fdf2-4f7b-88d4-39722ae50e50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-49-8b7b4e4a07b6>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-49-8b7b4e4a07b6>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.0212\n",
      "Epoch 2/3, Loss: 0.0161\n",
      "Epoch 3/3, Loss: 0.0205\n"
     ]
    }
   ],
   "source": [
    "input_size = 4*L # Define your input size based on the number of pixels in an image\n",
    "output_size = 3  # Assuming RGB prediction\n",
    "\n",
    "\n",
    "model_hd = MLP(input_size,256,128,64, output_size)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model_hd.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_hd(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfWaVvn36Gi5",
    "outputId": "d7b584ba-499b-4f27-ba03-b7d06eabd105"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-76934a52435c>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-50-76934a52435c>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0236\n"
     ]
    }
   ],
   "source": [
    "model_hd.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, targets in test_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        outputs = model_hd(inputs)\n",
    "\n",
    "        test_loss += criterion(outputs, targets)\n",
    "\n",
    "    average_test_loss = test_loss / len(test_dataloader)\n",
    "    print(f'Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHx2atoC6GmD"
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model_hd, '/content/drive/My Drive/model_mlp_hd.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSLO-FIi7FIu",
    "outputId": "f4d5bd1b-f06a-4d32-b040-0eafdfb45a52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-2253b964bab2>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "coor = [dataset[i][0] for i in range(len(dataset))]\n",
    "coor = torch.tensor(coor)\n",
    "\n",
    "model_hd.eval()\n",
    "\n",
    "batch_size = 64\n",
    "with torch.no_grad():\n",
    "  pre = []\n",
    "  for i in range(0, coor.size(0), batch_size):\n",
    "    batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n",
    "    batch_pred = model_hd(batch_coor)\n",
    "    pre.append(batch_pred)\n",
    "\n",
    "predicted_rgb = torch.cat(pre, dim=0)\n",
    "\n",
    "predicted_rgb = predicted_rgb.view(dataset.height, dataset.width, 3).numpy() * 255\n",
    "predicted_image = Image.fromarray(predicted_rgb.astype('uint8'))\n",
    "\n",
    "predicted_image.save('./mlp_hd.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N06KcM_o7FOL",
    "outputId": "569d1f26-c42a-42c1-9efb-4ad07fc99269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0376,  0.9993,  0.0752,  0.9972,  0.9436, -0.3311, -0.6248, -0.7808],\n",
      "        [ 1.0000,  0.0065,  0.0131, -0.9999,  0.9929, -0.1190, -0.2363, -0.9717],\n",
      "        [-0.3675, -0.9300,  0.6836,  0.7299,  0.9492,  0.3145,  0.5972, -0.8021],\n",
      "        [-0.3507,  0.9365, -0.6569,  0.7540,  0.9387,  0.3448,  0.6472, -0.7623],\n",
      "        [-0.3323,  0.9432, -0.6268,  0.7792, -0.0262,  0.9997, -0.0523,  0.9986],\n",
      "        [-0.9978,  0.0670, -0.1338, -0.9910,  0.9969, -0.0785, -0.1564, -0.9877],\n",
      "        [ 0.9393,  0.3430,  0.6445, -0.7646, -0.4436, -0.8962,  0.7951,  0.6065],\n",
      "        [-0.9882,  0.1532, -0.3028, -0.9531, -0.9962,  0.0872, -0.1736, -0.9848],\n",
      "        [-0.9788,  0.2047, -0.4007, -0.9162,  0.9799, -0.1994, -0.3907, -0.9205],\n",
      "        [-0.4829, -0.8757,  0.8457,  0.5336, -0.8802, -0.4746,  0.8355, -0.5495],\n",
      "        [-0.9382, -0.3461,  0.6494, -0.7604, -0.8004, -0.5995,  0.9596, -0.2812],\n",
      "        [ 0.9960, -0.0899, -0.1790, -0.9838, -0.0756, -0.9971,  0.1507,  0.9886],\n",
      "        [ 0.8997, -0.4364, -0.7853, -0.6191, -0.2108,  0.9775, -0.4120,  0.9112],\n",
      "        [ 0.2143, -0.9768, -0.4187,  0.9081,  0.4094, -0.9124, -0.7470,  0.6648],\n",
      "        [-0.9841,  0.1774, -0.3492, -0.9371, -0.4617,  0.8870, -0.8192,  0.5736],\n",
      "        [-0.9239,  0.3827, -0.7071, -0.7071,  0.9135,  0.4067,  0.7431, -0.6691]],\n",
      "       dtype=torch.float64) tensor([[0.0078, 0.3843, 0.5412],\n",
      "        [0.0902, 0.3843, 0.6118],\n",
      "        [0.7961, 0.6980, 0.1294],\n",
      "        [0.0000, 0.2510, 0.4078],\n",
      "        [0.9922, 0.9843, 0.9020],\n",
      "        [0.0706, 0.3098, 0.3137],\n",
      "        [0.2784, 0.3098, 0.5020],\n",
      "        [0.2588, 0.2471, 0.3294],\n",
      "        [0.0549, 0.3098, 0.3882],\n",
      "        [0.2471, 0.1725, 0.2824],\n",
      "        [0.7020, 0.5647, 0.6588],\n",
      "        [0.6510, 0.5373, 0.6157],\n",
      "        [0.0039, 0.2353, 0.4627],\n",
      "        [0.0039, 0.0000, 0.0196],\n",
      "        [0.9686, 0.9137, 0.8706],\n",
      "        [0.0039, 0.2667, 0.2902]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "L=2\n",
    "\n",
    "dataset = ImagePixelDatasetHigherDimention(path,L)\n",
    "dataloader = DataLoader(dataset,batch_size=16,shuffle=True)\n",
    "train_feature, train_labels = next(iter(dataloader))\n",
    "print(train_feature, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dZdpP_9Hw5B"
   },
   "outputs": [],
   "source": [
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5jFE7nPHw9G"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uz1RAwN9Hw_1",
    "outputId": "a642f827-40a3-4ed1-9582-712e6fc00136"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/414720 [00:00<?, ?it/s]<ipython-input-19-9ae4fbe6dfb1>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-19-9ae4fbe6dfb1>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n",
      "100%|██████████| 414720/414720 [14:21<00:00, 481.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.0299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414720/414720 [14:15<00:00, 484.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.0245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414720/414720 [14:03<00:00, 491.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.0076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_size = 4*L # Define your input size based on the number of pixels in an image\n",
    "output_size = 3  # Assuming RGB prediction\n",
    "\n",
    "\n",
    "model_hd_l2 = MLP4(input_size,256,128,64, output_size)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model_hd_l2.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in tqdm(train_dataloader):\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_hd_l2(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "JXkdpfq2ITuF",
    "outputId": "c3b4cc20-78d4-4b40-a80a-3e8ebb7a2c22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-00bcd8139e86>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-20-00bcd8139e86>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0269\n"
     ]
    }
   ],
   "source": [
    "model_hd_l2.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, targets in test_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        outputs = model_hd_l2(inputs)\n",
    "\n",
    "        test_loss += criterion(outputs, targets)\n",
    "\n",
    "    average_test_loss = test_loss / len(test_dataloader)\n",
    "    print(f'Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4uLaUyGGITw-"
   },
   "outputs": [],
   "source": [
    "torch.save(model_hd_l2, '/content/drive/My Drive/model_mlp_hd_l2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jLu2aaPIfIG"
   },
   "outputs": [],
   "source": [
    "coor = [dataset[i][0] for i in range(len(dataset))]\n",
    "coor = torch.tensor(coor)\n",
    "\n",
    "model_hd_l2.eval()\n",
    "\n",
    "batch_size = 64\n",
    "with torch.no_grad():\n",
    "  pre = []\n",
    "  for i in range(0, coor.size(0), batch_size):\n",
    "    batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n",
    "    batch_pred = model_hd_l2(batch_coor)\n",
    "    pre.append(batch_pred)\n",
    "\n",
    "predicted_rgb = torch.cat(pre, dim=0)\n",
    "\n",
    "predicted_rgb = predicted_rgb.view(dataset.height, dataset.width, 3).numpy() * 255\n",
    "predicted_image = Image.fromarray(predicted_rgb.astype('uint8'))\n",
    "\n",
    "predicted_image.save('./mlp_hd_l2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbD375Hl7BCs",
    "outputId": "fd1f61e1-e577-4879-e018-3433f9660409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4394, -0.8983,  0.3475, -0.9377],\n",
      "        [ 0.5364, -0.8440,  0.7490, -0.6626],\n",
      "        [-0.9989,  0.0474,  0.7451,  0.6670],\n",
      "        [-0.5141,  0.8577, -0.9853, -0.1708],\n",
      "        [ 0.1257,  0.9921,  0.3090, -0.9511],\n",
      "        [ 0.4438, -0.8961,  0.1103,  0.9939],\n",
      "        [ 0.7914, -0.6114,  0.4226,  0.9063],\n",
      "        [-0.9847, -0.1742, -0.6926, -0.7214],\n",
      "        [-0.5253,  0.8509, -0.0029,  1.0000],\n",
      "        [ 0.9967,  0.0817,  0.8802, -0.4746],\n",
      "        [-0.1516, -0.9884,  0.7771, -0.6293],\n",
      "        [-0.2271,  0.9739, -0.4173,  0.9088],\n",
      "        [-0.3584,  0.9336, -0.9724, -0.2334],\n",
      "        [ 0.4685, -0.8835,  0.0436,  0.9990],\n",
      "        [ 0.9521,  0.3059,  0.2193,  0.9757],\n",
      "        [-0.2919,  0.9565,  0.8208,  0.5712]], dtype=torch.float64) tensor([[0.2510, 0.2824, 0.1922],\n",
      "        [0.0314, 0.0627, 0.0196],\n",
      "        [0.3255, 0.4157, 0.3529],\n",
      "        [0.2039, 0.1608, 0.1451],\n",
      "        [0.1137, 0.1608, 0.2627],\n",
      "        [0.0078, 0.2157, 0.5137],\n",
      "        [0.0118, 0.2000, 0.4431],\n",
      "        [0.1490, 0.1490, 0.1804],\n",
      "        [0.6588, 0.7725, 0.8275],\n",
      "        [0.0039, 0.0078, 0.0157],\n",
      "        [0.6078, 0.4745, 0.1843],\n",
      "        [0.0510, 0.1569, 0.2627],\n",
      "        [0.1059, 0.1137, 0.1608],\n",
      "        [0.0039, 0.2039, 0.4275],\n",
      "        [0.0000, 0.1098, 0.2980],\n",
      "        [0.0000, 0.1059, 0.2745]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "L=1\n",
    "\n",
    "dataset = ImagePixelDatasetHigherDimention(path,L)\n",
    "dataloader = DataLoader(dataset,batch_size=16,shuffle=True)\n",
    "train_feature, train_labels = next(iter(dataloader))\n",
    "print(train_feature, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "X0sUvSpeg3KH"
   },
   "outputs": [],
   "source": [
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LF12bWIjg9mK"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1x08ePog9ok",
    "outputId": "0e14c427-31d8-4182-f9e9-4d7ea91b4ccc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/414720 [00:00<?, ?it/s]<ipython-input-18-e7b514f68dda>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-18-e7b514f68dda>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n",
      "100%|██████████| 414720/414720 [14:06<00:00, 489.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.0292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414720/414720 [13:51<00:00, 498.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414720/414720 [13:39<00:00, 506.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_size = 4*L # Define your input size based on the number of pixels in an image\n",
    "output_size = 3  # Assuming RGB prediction\n",
    "\n",
    "\n",
    "model_hd_l1 = MLP4(input_size,256,128,64, output_size)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model_hd_l1.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in tqdm(train_dataloader):\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_hd_l1(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7kOypY3g9rY",
    "outputId": "e126f515-f529-4de6-917b-aedb6206393a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-01dd8aa317fa>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-19-01dd8aa317fa>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0292\n"
     ]
    }
   ],
   "source": [
    "model_hd_l1.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, targets in test_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        outputs = model_hd_l1(inputs)\n",
    "\n",
    "        test_loss += criterion(outputs, targets)\n",
    "\n",
    "    average_test_loss = test_loss / len(test_dataloader)\n",
    "    print(f'Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQtSDUmxh06Q",
    "outputId": "798f3f84-f159-427b-deb0-aed1f6e43719"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-834b05bb3df6>:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  coor = torch.tensor(coor)\n",
      "<ipython-input-20-834b05bb3df6>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "coor = [dataset[i][0] for i in range(len(dataset))]\n",
    "coor = torch.tensor(coor)\n",
    "\n",
    "model_hd_l1.eval()\n",
    "\n",
    "batch_size = 64\n",
    "with torch.no_grad():\n",
    "  pre = []\n",
    "  for i in range(0, coor.size(0), batch_size):\n",
    "    batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n",
    "    batch_pred = model_hd_l1(batch_coor)\n",
    "    pre.append(batch_pred)\n",
    "\n",
    "predicted_rgb = torch.cat(pre, dim=0)\n",
    "\n",
    "predicted_rgb = predicted_rgb.view(dataset.height, dataset.width, 3).numpy() * 255\n",
    "predicted_image = Image.fromarray(predicted_rgb.astype('uint8'))\n",
    "\n",
    "predicted_image.save('./mlp_hd_l1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lku8NyCNiWS7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
