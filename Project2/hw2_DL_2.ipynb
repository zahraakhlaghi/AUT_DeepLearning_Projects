{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOtXY9WAPt0C"
   },
   "outputs": [],
   "source": [
    "from turtle import width\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOztUB3FZWK8"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ImagePixelDatasetHigherDimention(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self,image_path,L):\n",
    "    self.image = np.array(Image.open(image_path))\n",
    "    self.height,self.width,_ = self.image.shape\n",
    "    self.L = L\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.height * self.width\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "    y = idx // self.width\n",
    "    x = idx % self.width\n",
    "    color = self.image[y,x]/255\n",
    "\n",
    "    y = np.interp(y,[0,self.height],[-1,1])\n",
    "    x = np.interp(x,[0,self.width],[-1,1])\n",
    "\n",
    "    input = []\n",
    "\n",
    "    for l in range(0,self.L):\n",
    "        input.append(np.sin(2**l * np.pi * x))\n",
    "        input.append(np.cos(2**l * np.pi * x))\n",
    "\n",
    "    for l in range(0,self.L):\n",
    "        input.append(np.sin(2**l * np.pi * y))\n",
    "        input.append(np.cos(2**l * np.pi * y))\n",
    "\n",
    "\n",
    "    return np.array(input), color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggV22_IKZZ0I",
    "outputId": "60564510-6021-42ad-ac62-3bb8012dd232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "path = \"/content/drive/My Drive/sifnos-greece-3840x2160-12799.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuLQPx8wZZ2l",
    "outputId": "710473be-baac-4f7f-9273-c407af7e310a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8032,  0.5957, -0.9569, -0.2903,  0.5556, -0.8315, -0.7566, -0.6539,\n",
      "          0.9894, -0.1449, -0.2868, -0.9580],\n",
      "        [ 0.9988,  0.0491,  0.0980, -0.9952, -0.1951,  0.9808, -0.1908, -0.9816,\n",
      "          0.3746,  0.9272,  0.6947,  0.7193],\n",
      "        [ 0.7625,  0.6470,  0.9866, -0.1629, -0.3214, -0.9469, -0.7735, -0.6338,\n",
      "          0.9805, -0.1965, -0.3854, -0.9228],\n",
      "        [-0.9771, -0.2127,  0.4157, -0.9095, -0.7561,  0.6544, -0.9026, -0.4305,\n",
      "          0.7771, -0.6293, -0.9781, -0.2079],\n",
      "        [ 0.1710,  0.9853,  0.3369,  0.9415,  0.6344,  0.7730,  0.3611, -0.9325,\n",
      "         -0.6734,  0.7392, -0.9957,  0.0929],\n",
      "        [ 0.9376, -0.3477, -0.6519, -0.7583,  0.9887,  0.1500,  0.8587, -0.5125,\n",
      "         -0.8802, -0.4746,  0.8355, -0.5495],\n",
      "        [-0.9437,  0.3307, -0.6242, -0.7812,  0.9753,  0.2207, -0.9811,  0.1937,\n",
      "         -0.3800, -0.9250,  0.7030,  0.7112],\n",
      "        [-0.5127, -0.8586,  0.8804,  0.4743,  0.8351, -0.5501, -0.1507,  0.9886,\n",
      "         -0.2979,  0.9546, -0.5688,  0.8225],\n",
      "        [ 0.9700,  0.2430,  0.4714, -0.8819, -0.8315,  0.5556, -0.6450,  0.7642,\n",
      "         -0.9858,  0.1679, -0.3311, -0.9436],\n",
      "        [-0.0719,  0.9974, -0.1435,  0.9897, -0.2840,  0.9588,  0.9967,  0.0814,\n",
      "          0.1622, -0.9868, -0.3201,  0.9474],\n",
      "        [-0.9847,  0.1742, -0.3430, -0.9393,  0.6445,  0.7646, -0.5568, -0.8307,\n",
      "          0.9250,  0.3800,  0.7030, -0.7112],\n",
      "        [ 0.3183, -0.9480, -0.6036,  0.7973, -0.9625,  0.2714, -0.8870,  0.4617,\n",
      "         -0.8192, -0.5736,  0.9397, -0.3420],\n",
      "        [ 0.5569,  0.8306,  0.9251,  0.3797,  0.7025, -0.7117, -0.4358,  0.9001,\n",
      "         -0.7844,  0.6202, -0.9730, -0.2306],\n",
      "        [-0.3400,  0.9404, -0.6394,  0.7688, -0.9833,  0.1822,  0.5100,  0.8601,\n",
      "          0.8774,  0.4797,  0.8418, -0.5398],\n",
      "        [-0.6394,  0.7688, -0.9833,  0.1822, -0.3584, -0.9336, -0.1045, -0.9945,\n",
      "          0.2079,  0.9781,  0.4067,  0.9135],\n",
      "        [-0.5057,  0.8627, -0.8725,  0.4886, -0.8526, -0.5225,  0.2756, -0.9613,\n",
      "         -0.5299,  0.8480, -0.8988,  0.4384]], dtype=torch.float64) tensor([[0.1804, 0.2000, 0.2235],\n",
      "        [0.2039, 0.2980, 0.3137],\n",
      "        [0.0980, 0.1608, 0.1176],\n",
      "        [0.1412, 0.1686, 0.2000],\n",
      "        [0.0000, 0.0039, 0.0235],\n",
      "        [0.2353, 0.1373, 0.0157],\n",
      "        [0.1961, 0.2039, 0.2000],\n",
      "        [0.5020, 0.2824, 0.3255],\n",
      "        [0.2549, 0.3333, 0.2902],\n",
      "        [0.0000, 0.2157, 0.4353],\n",
      "        [0.3059, 0.3922, 0.3804],\n",
      "        [0.6235, 0.6588, 0.6863],\n",
      "        [0.7020, 0.4314, 0.3176],\n",
      "        [0.0235, 0.1961, 0.3020],\n",
      "        [0.0824, 0.0941, 0.1686],\n",
      "        [0.0510, 0.4157, 0.5451]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "L=3\n",
    "\n",
    "dataset = ImagePixelDatasetHigherDimention(path,L)\n",
    "dataloader = DataLoader(dataset,batch_size=16,shuffle=True)\n",
    "train_feature, train_labels = next(iter(dataloader))\n",
    "print(train_feature, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jck2SakuZZ5P"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1,hidden_size2,hidden_size3, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size3, output_size)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.relu4(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnLyUUGeZZ8D"
   },
   "outputs": [],
   "source": [
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4mtINojZhr_"
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xunM1L9ZhuR",
    "outputId": "cdaf2a46-c160-47cd-837f-b1971516e407"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-cf5e2d0730dc>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-18-cf5e2d0730dc>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.09403299540281296\n",
      "---Duration: 8925.825008869171 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "input_size = 4*L # Define your input size based on the number of pixels in an image\n",
    "output_size = 3  # Assuming RGB prediction\n",
    "\n",
    "\n",
    "model_hd_LBFGS = MLP(input_size,256,128,64, output_size)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = torch.optim.LBFGS(model_hd_LBFGS.parameters(), lr=1,max_eval=30,max_iter=3)\n",
    "optimizer = torch.optim.LBFGS(model_hd_LBFGS.parameters(), lr=0.01, max_iter=20, max_eval=None, tolerance_grad=1e-5, tolerance_change=1e-9, history_size=100)\n",
    "\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_hd_LBFGS(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "num_epochs =1\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # Print the loss at each iteration\n",
    "    print(f'Epoch {epoch + 1}, Loss: {closure().item()}')\n",
    "\n",
    "print(\"---Duration: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qE6e9G16ZhxE",
    "outputId": "9e7faca4-6caf-45ea-9108-072a7f3de876"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-090a907e8146>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-19-090a907e8146>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1025\n"
     ]
    }
   ],
   "source": [
    "model_hd_LBFGS.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, targets in test_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        outputs = model_hd_LBFGS(inputs)\n",
    "\n",
    "        test_loss += criterion(outputs, targets)\n",
    "\n",
    "    average_test_loss = test_loss / len(test_dataloader)\n",
    "    print(f'Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3H1m-8C6ZZ-g"
   },
   "outputs": [],
   "source": [
    "torch.save(model_hd_LBFGS, '/content/drive/My Drive/model_mlp_hd_LBFGS.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QZl4ZsiaaW0T",
    "outputId": "7fc75ea6-70df-476e-ec81-1a701da77242"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-e04e76a9ead6>:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  coor = torch.tensor(coor)\n",
      "<ipython-input-21-e04e76a9ead6>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "coor = [dataset[i][0] for i in range(len(dataset))]\n",
    "coor = torch.tensor(coor)\n",
    "\n",
    "model_hd_LBFGS.eval()\n",
    "\n",
    "batch_size = 64\n",
    "with torch.no_grad():\n",
    "  pre = []\n",
    "  for i in range(0, coor.size(0), batch_size):\n",
    "    batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n",
    "    batch_pred = model_hd_LBFGS(batch_coor)\n",
    "    pre.append(batch_pred)\n",
    "\n",
    "predicted_rgb = torch.cat(pre, dim=0)\n",
    "\n",
    "predicted_rgb = predicted_rgb.view(dataset.height, dataset.width, 3).numpy() * 255\n",
    "predicted_image = Image.fromarray(predicted_rgb.astype('uint8'))\n",
    "\n",
    "predicted_image.save('./mlp_hd_LBFGS.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-n-nVKoAaW2g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIk5zc2haW5M"
   },
   "outputs": [],
   "source": [
    "from turtle import width\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSK0lI0qKYF9"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ImagePixelDatasetHigherDimention(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self,image_path,L):\n",
    "    self.image = np.array(Image.open(image_path))\n",
    "    self.height,self.width,_ = self.image.shape\n",
    "    self.L = L\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.height * self.width\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "    y = idx // self.width\n",
    "    x = idx % self.width\n",
    "    color = self.image[y,x]/255\n",
    "\n",
    "    y = np.interp(y,[0,self.height],[-1,1])\n",
    "    x = np.interp(x,[0,self.width],[-1,1])\n",
    "\n",
    "    input = []\n",
    "\n",
    "    for l in range(0,self.L):\n",
    "        input.append(np.sin(2**l * np.pi * x))\n",
    "        input.append(np.cos(2**l * np.pi * x))\n",
    "\n",
    "    for l in range(0,self.L):\n",
    "        input.append(np.sin(2**l * np.pi * y))\n",
    "        input.append(np.cos(2**l * np.pi * y))\n",
    "\n",
    "\n",
    "    return np.array(input), color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9nONENG9KYIq",
    "outputId": "b194148e-32d0-40d8-d214-1b9d19e153b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "path = \"/content/drive/My Drive/sifnos-greece-3840x2160-12799.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUHcboNWKYLI",
    "outputId": "b8401315-05ed-4a7b-b4f4-52b00b34c04d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8594, -0.5113, -0.8788, -0.4772,  0.8387, -0.5446, -0.6626,  0.7490,\n",
      "         -0.9925,  0.1219, -0.2419, -0.9703],\n",
      "        [ 0.8865,  0.4627,  0.8204, -0.5718, -0.9382, -0.3461,  0.2560, -0.9667,\n",
      "         -0.4950,  0.8689, -0.8601,  0.5100],\n",
      "        [ 0.0442,  0.9990,  0.0882,  0.9961,  0.1758,  0.9844, -0.7604, -0.6494,\n",
      "          0.9877, -0.1564, -0.3090, -0.9511],\n",
      "        [-0.0866,  0.9962, -0.1726,  0.9850, -0.3400,  0.9404,  0.4924, -0.8704,\n",
      "         -0.8572,  0.5150, -0.8829, -0.4695],\n",
      "        [ 0.9974,  0.0719,  0.1435, -0.9897, -0.2840,  0.9588, -0.9918,  0.1276,\n",
      "         -0.2532, -0.9674,  0.4899,  0.8718],\n",
      "        [-0.3751, -0.9270,  0.6954,  0.7186,  0.9995,  0.0327,  0.1851, -0.9827,\n",
      "         -0.3638,  0.9315, -0.6777,  0.7353],\n",
      "        [-0.8241, -0.5664,  0.9336, -0.3584, -0.6691, -0.7431, -0.5373, -0.8434,\n",
      "          0.9063,  0.4226,  0.7660, -0.6428],\n",
      "        [-0.8138,  0.5811, -0.9459, -0.3245,  0.6139, -0.7894, -0.9877,  0.1564,\n",
      "         -0.3090, -0.9511,  0.5878,  0.8090],\n",
      "        [ 0.5811,  0.8138,  0.9459,  0.3245,  0.6139, -0.7894, -0.0116,  0.9999,\n",
      "         -0.0233,  0.9997, -0.0465,  0.9989],\n",
      "        [-0.3369,  0.9415, -0.6344,  0.7730, -0.9808,  0.1951,  0.1564,  0.9877,\n",
      "          0.3090,  0.9511,  0.5878,  0.8090],\n",
      "        [-0.6884, -0.7254,  0.9986,  0.0523,  0.1045, -0.9945,  0.7844,  0.6202,\n",
      "          0.9730, -0.2306, -0.4488, -0.8936],\n",
      "        [-0.9550, -0.2965,  0.5664, -0.8241, -0.9336,  0.3584, -0.9877, -0.1564,\n",
      "          0.3090, -0.9511, -0.5878,  0.8090],\n",
      "        [ 0.9700, -0.2430, -0.4714, -0.8819,  0.8315,  0.5556,  0.9744, -0.2250,\n",
      "         -0.4384, -0.8988,  0.7880,  0.6157],\n",
      "        [ 0.9095, -0.4157, -0.7561, -0.6544,  0.9897, -0.1435,  0.9848, -0.1736,\n",
      "         -0.3420, -0.9397,  0.6428,  0.7660],\n",
      "        [ 0.0785,  0.9969,  0.1564,  0.9877,  0.3090,  0.9511, -0.6202,  0.7844,\n",
      "         -0.9730,  0.2306, -0.4488, -0.8936],\n",
      "        [-0.8611,  0.5085, -0.8757, -0.4829,  0.8457, -0.5336, -0.8910, -0.4540,\n",
      "          0.8090, -0.5878, -0.9511, -0.3090]], dtype=torch.float64) tensor([[0.0078, 0.0980, 0.2235],\n",
      "        [0.1020, 0.0745, 0.1373],\n",
      "        [0.1529, 0.1725, 0.1569],\n",
      "        [0.0000, 0.3882, 0.4824],\n",
      "        [0.4784, 0.5529, 0.5765],\n",
      "        [0.3137, 0.3294, 0.2235],\n",
      "        [0.1922, 0.1412, 0.2078],\n",
      "        [0.2471, 0.2235, 0.2706],\n",
      "        [0.4706, 0.4353, 0.4392],\n",
      "        [0.2980, 0.3647, 0.5373],\n",
      "        [0.2431, 0.2706, 0.4824],\n",
      "        [0.2902, 0.2588, 0.3098],\n",
      "        [0.1176, 0.4392, 0.6275],\n",
      "        [0.1020, 0.4314, 0.7059],\n",
      "        [0.3176, 0.3412, 0.3020],\n",
      "        [0.0000, 0.1020, 0.0980]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "L=3\n",
    "\n",
    "dataset = ImagePixelDatasetHigherDimention(path,L)\n",
    "dataloader = DataLoader(dataset,batch_size=16,shuffle=True)\n",
    "train_feature, train_labels = next(iter(dataloader))\n",
    "print(train_feature, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIWg6iNyKYNq"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class MLP3(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1,hidden_size2, output_size):\n",
    "        super(MLP3, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class MLP4(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1,hidden_size2,hidden_size3, output_size):\n",
    "        super(MLP4, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size3, output_size)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.relu4(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a268Wt98KYQA"
   },
   "outputs": [],
   "source": [
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "batch_size = 4096\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WjUiygX9KYSS",
    "outputId": "a1c5deb9-823d-49c1-e48e-eb5bb948807e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]<ipython-input-11-a652445eab4f>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-11-a652445eab4f>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n",
      " 33%|███▎      | 1/3 [07:47<15:34, 467.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.02762218751013279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [15:22<07:40, 460.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.026545308530330658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [22:58<00:00, 459.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.024592235684394836\n",
      "---Duration: 1378.901659488678 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "input_size = 4*L # Define your input size based on the number of pixels in an image\n",
    "output_size = 3  # Assuming RGB prediction\n",
    "\n",
    "model_256_LBFGS = MLP(input_size, 256, output_size)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = torch.optim.LBFGS(model_hd_LBFGS.parameters(), lr=1,max_eval=30,max_iter=3)\n",
    "optimizer = torch.optim.LBFGS(model_256_LBFGS.parameters(), lr=0.01, max_iter=20, max_eval=None, tolerance_grad=1e-5, tolerance_change=1e-9, history_size=100)\n",
    "\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_256_LBFGS(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "num_epochs =3\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # Print the loss at each iteration\n",
    "    print(f'Epoch {epoch + 1}, Loss: {closure().item()}')\n",
    "\n",
    "print(\"---Duration: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-A0-cFVPrB7",
    "outputId": "9c042c89-7222-4d25-dc7c-9d30e9bb7c9a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-49818b9c1ea8>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-12-49818b9c1ea8>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0263\n"
     ]
    }
   ],
   "source": [
    "model_256_LBFGS.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, targets in test_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        outputs = model_256_LBFGS(inputs)\n",
    "\n",
    "        test_loss += criterion(outputs, targets)\n",
    "\n",
    "    average_test_loss = test_loss / len(test_dataloader)\n",
    "    print(f'Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FbvRXL1PrEY"
   },
   "outputs": [],
   "source": [
    "torch.save(model_256_LBFGS, '/content/drive/My Drive/model_mlp_256_LBFGS.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpJ7_pl0PrHK",
    "outputId": "10344343-2c5a-4ae5-a9de-a277aee017d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-57afc6420671>:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  coor = torch.tensor(coor)\n",
      "<ipython-input-14-57afc6420671>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "coor = [dataset[i][0] for i in range(len(dataset))]\n",
    "coor = torch.tensor(coor)\n",
    "\n",
    "model_256_LBFGS.eval()\n",
    "\n",
    "batch_size = 64\n",
    "with torch.no_grad():\n",
    "  pre = []\n",
    "  for i in range(0, coor.size(0), batch_size):\n",
    "    batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n",
    "    batch_pred = model_256_LBFGS(batch_coor)\n",
    "    pre.append(batch_pred)\n",
    "\n",
    "predicted_rgb = torch.cat(pre, dim=0)\n",
    "\n",
    "predicted_rgb = predicted_rgb.view(dataset.height, dataset.width, 3).numpy() * 255\n",
    "predicted_image = Image.fromarray(predicted_rgb.astype('uint8'))\n",
    "\n",
    "predicted_image.save('./mlp_256_LBFGS.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9BUJtbDGaD5Q",
    "outputId": "80168951-c27f-4400-aed3-064adf81a0c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]<ipython-input-23-67d1607c9789>:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-23-67d1607c9789>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n",
      " 33%|███▎      | 1/3 [16:13<32:27, 973.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.02550884522497654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [32:46<16:25, 985.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.02555813454091549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [49:08<00:00, 982.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.023976227268576622\n",
      "---Duration: 2948.0328373908997 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batch_size = 4096\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "input_size = 4*L # Define your input size based on the number of pixels in an image\n",
    "output_size = 3  # Assuming RGB prediction\n",
    "\n",
    "model_256_128_LBFGS = MLP3(input_size, 256,128, output_size)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = torch.optim.LBFGS(model_hd_LBFGS.parameters(), lr=1,max_eval=30,max_iter=3)\n",
    "optimizer = torch.optim.LBFGS(model_256_128_LBFGS.parameters(), lr=0.01, max_iter=20, max_eval=None, tolerance_grad=1e-5, tolerance_change=1e-9, history_size=100)\n",
    "\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_256_128_LBFGS(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "num_epochs =3\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # Print the loss at each iteration\n",
    "    print(f'Epoch {epoch + 1}, Loss: {closure().item()}')\n",
    "\n",
    "print(\"---Duration: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dem8y6XJaD7w",
    "outputId": "abd17527-2274-4fdd-902d-0843f7a27edb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-65d67874c9db>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-24-65d67874c9db>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0252\n"
     ]
    }
   ],
   "source": [
    "model_256_128_LBFGS.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, targets in test_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        outputs = model_256_128_LBFGS(inputs)\n",
    "\n",
    "        test_loss += criterion(outputs, targets)\n",
    "\n",
    "    average_test_loss = test_loss / len(test_dataloader)\n",
    "    print(f'Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dC6u9TewaD-k"
   },
   "outputs": [],
   "source": [
    "torch.save(model_256_128_LBFGS, '/content/drive/My Drive/model_mlp_256_128_LBFGS.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCWt1_eVavBh",
    "outputId": "ea66f1a0-8827-4a1a-a150-db8f2d540d5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-d2577d591004>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "coor = [dataset[i][0] for i in range(len(dataset))]\n",
    "coor = torch.tensor(coor)\n",
    "\n",
    "model_256_128_LBFGS.eval()\n",
    "\n",
    "batch_size = 64\n",
    "with torch.no_grad():\n",
    "  pre = []\n",
    "  for i in range(0, coor.size(0), batch_size):\n",
    "    batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n",
    "    batch_pred = model_256_128_LBFGS(batch_coor)\n",
    "    pre.append(batch_pred)\n",
    "\n",
    "predicted_rgb = torch.cat(pre, dim=0)\n",
    "\n",
    "predicted_rgb = predicted_rgb.view(dataset.height, dataset.width, 3).numpy() * 255\n",
    "predicted_image = Image.fromarray(predicted_rgb.astype('uint8'))\n",
    "\n",
    "predicted_image.save('./mlp_256_128_LBFGS.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dV5M04o5fjmh",
    "outputId": "323807e7-93be-4358-e219-94dbd1a875ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]<ipython-input-27-e9ebb0450f5a>:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-27-e9ebb0450f5a>:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n",
      " 33%|███▎      | 1/3 [18:22<36:44, 1102.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.02549625001847744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [36:58<18:30, 1110.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.02551054023206234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [55:22<00:00, 1107.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.024893099442124367\n",
      "---Duration: 3322.069444656372 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4096\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "input_size = 4*L # Define your input size based on the number of pixels in an image\n",
    "output_size = 3  # Assuming RGB prediction\n",
    "\n",
    "model_256_128_64_LBFGS =  MLP4(input_size,256,128,64, output_size)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = torch.optim.LBFGS(model_hd_LBFGS.parameters(), lr=1,max_eval=30,max_iter=3)\n",
    "optimizer = torch.optim.LBFGS(model_256_128_64_LBFGS.parameters(), lr=0.01, max_iter=20, max_eval=None, tolerance_grad=1e-5, tolerance_change=1e-9, history_size=100)\n",
    "\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_256_128_64_LBFGS(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "num_epochs =3\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # Print the loss at each iteration\n",
    "    print(f'Epoch {epoch + 1}, Loss: {closure().item()}')\n",
    "\n",
    "print(\"---Duration: %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aG7xGpZ1fjjx",
    "outputId": "71871b99-1ea9-4c9e-efd3-a8f6c356da9a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-94df87380c68>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs,dtype=torch.float32)\n",
      "<ipython-input-28-94df87380c68>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0249\n"
     ]
    }
   ],
   "source": [
    "model_256_128_64_LBFGS.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, targets in test_dataloader:\n",
    "        # Flatten the inputs if needed\n",
    "\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        targets = torch.tensor(targets,dtype=torch.float32)\n",
    "\n",
    "        outputs = model_256_128_64_LBFGS(inputs)\n",
    "\n",
    "        test_loss += criterion(outputs, targets)\n",
    "\n",
    "    average_test_loss = test_loss / len(test_dataloader)\n",
    "    print(f'Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZfX9BVVZfjhz"
   },
   "outputs": [],
   "source": [
    "torch.save(model_256_128_64_LBFGS, '/content/drive/My Drive/model_mlp_256_128_64_LBFGS.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q52LsHTNfjSt",
    "outputId": "832274c7-ba19-4842-c5a0-af802ed8ebab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-335d0563e956>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "coor = [dataset[i][0] for i in range(len(dataset))]\n",
    "coor = torch.tensor(coor)\n",
    "\n",
    "model_256_128_64_LBFGS.eval()\n",
    "\n",
    "batch_size = 64\n",
    "with torch.no_grad():\n",
    "  pre = []\n",
    "  for i in range(0, coor.size(0), batch_size):\n",
    "    batch_coor = torch.tensor(coor[i:i+batch_size, :],dtype=torch.float32)\n",
    "    batch_pred = model_256_128_64_LBFGS(batch_coor)\n",
    "    pre.append(batch_pred)\n",
    "\n",
    "predicted_rgb = torch.cat(pre, dim=0)\n",
    "\n",
    "predicted_rgb = predicted_rgb.view(dataset.height, dataset.width, 3).numpy() * 255\n",
    "predicted_image = Image.fromarray(predicted_rgb.astype('uint8'))\n",
    "\n",
    "predicted_image.save('./mlp_256_128_64_LBFGS.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_0WHylcgvcx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
